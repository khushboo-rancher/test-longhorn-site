<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Longhorn Test Cases on Longhorn Manual Test Cases</title>
    <link>https://khushboo-rancher/test-longhorn-site/</link>
    <description>Recent content in Longhorn Test Cases on Longhorn Manual Test Cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://khushboo-rancher/test-longhorn-site/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Deployment of Longhorn</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/deployment/</guid>
      <description>Installation Longhorn v1.1.2 and above - Support Kubernetes 1.18+
Longhorn v1.0.0 to v1.1.1 - Support Kubernetes 1.14+. Default 1.16+
  Install using Rancher Apps &amp;amp; MarketPlace App (Default)
  Install using Helm chart from https://github.com/longhorn/longhorn/tree/master/chart
  Install using YAML from https://github.com/longhorn/longhorn/blob/master/deploy/longhorn.yaml
  Note: Longhorn UI can scale to multiple instances for HA purposes.
Uninstallation Make sure all the CRDs and other resources are cleaned up, following the uninstallation instruction.</description>
    </item>
    
    <item>
      <title>2. UI</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/ui/</guid>
      <description>Accessibility of Longhorn UI    # Test Case Test Instructions     1. Access Longhorn UI using rancher proxy 1. Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.
2. Go to App, Click the launch app.
3. Select longhorn.
4. Select Rancher-Proxy under the Longhorn UI service.
5. Once the app is deployed successfully, click the /index.html link appears in App page.</description>
    </item>
    
    <item>
      <title>3. Volume</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/volume/</guid>
      <description>Test cases for Volume    # Test Case Test Instructions Expected Results     1 Check volume Details Prerequisite:
* Longhorn Nodes has node tags
* Node Disks has disk tags
* Backup target is set to NFS server, or S3 compatible target
1. Create a workload using Longhorn volume
2. Check volume details page
3. Create volume backup * Volume Details
* State should be Attached</description>
    </item>
    
    <item>
      <title>4. High Availability</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/high-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/high-availability/</guid>
      <description>Automation Tests    # Test Name Description Tags     1 test_engine_image_daemonset_restart Test restarting engine image daemonset
1. Get the default engine image
2. Create a volume and attach to the current node
3. Write random data to the volume and create a snapshot
4. Delete the engine image daemonset
5. Engine image daemonset should be recreated
6. In the meantime, validate the volume data to prove it&amp;rsquo;s still functional</description>
    </item>
    
    <item>
      <title>5. Kubernetes</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/kubernetes/</guid>
      <description>Dynamic provisioning with StorageClass   Can create and use volume using StorageClass
  Can create a new StorageClass use new parameters and it will take effect on the volume created by the storage class.
  If the PV reclaim policy is delete, once PVC and PV are deleted, Longhorn volume should be deleted.
  Static provisioning using Longhorn created PV/PVC   PVC can be used by the new workload</description>
    </item>
    
    <item>
      <title>6. Backup</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/backup/</guid>
      <description>Automation Tests    # Test name Description tag     1 test_backup Test basic backup
Setup:
1. Create a volume and attach to the current node
2. Run the test for all the available backupstores.
Steps:
1. Create a backup of volume
2. Restore the backup to a new volume
3. Attach the new volume and make sure the data is the same as the old one</description>
    </item>
    
    <item>
      <title>7. Node</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/node/</guid>
      <description>UI specific test cases    # Test Case Test Instructions Expected Results     1 Storage details * Prerequisites
* Longhorn Installed
1. Verify the allocated/used storage show the right data in node details page.
2. Create a volume of 20 GB and attach to a pod and verify the storage allocated/used is shown correctly. Without any volume, allocated should be 0 and on creating new volume it should be updated as per volume present.</description>
    </item>
    
    <item>
      <title>8. Scheduling</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/scheduling/</guid>
      <description>Manual Test    Test name Prerequisite Expectation     EKS across zone scheduling Prerequisite:
* EKS Cluster with 3 nodes across two AWS zones (zone#1, zone#2)
1. Create a volume with 2 replicas, and attach it to a node.
2. Delete a replica scheduled to each zone, repeat it few times
3. Scale volume replicas = 3
4. Scale volume replicas to 4 * Volume replicas should be scheduled one per AWS zone</description>
    </item>
    
    <item>
      <title>9. Upgrade</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/upgrade/</guid>
      <description>Automation tests
   # Test name Description Tags     1 test_upgrade Test Longhorn upgrade
1. Find the upgrade image tag
2. Create a volume, generate and write data into the volume.
3. Keep the volume attached, then upgrade Longhorn system.
4. Detach the volume.
5. Upgrade the volume to the updated engine image.
6. Attach the volume and verify data Upgrade   2 test_engine_image Test Engine Image deployment</description>
    </item>
    
    <item>
      <title>[#1279](https://github.com/longhorn/longhorn/issues/1279) DR volume live upgrade and rebuild</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/dr-volume-live-upgrade-and-rebuild/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/dr-volume-live-upgrade-and-rebuild/</guid>
      <description>Launch Longhorn at the previous version. Launch a pod with Longhorn volume. Write data to the volume and take the 1st backup. Create 2 DR volumes from the 1st backup. Shutdown the pod and wait for the original volume detached. Expand the original volume and wait for the expansion complete. Write data to the original volume and take the 2nd backup. (Make sure the total data size is larger than the original volume size so that there is date written to the expanded part.</description>
    </item>
    
    <item>
      <title>[#1326](https://github.com/longhorn/longhorn/issues/1326) concurrent backup creation &amp; deletion</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/concurrent-backup-creation-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/concurrent-backup-creation-deletion/</guid>
      <description>This one is a special case, were the volume only contains 1 backup, which the user requests to delete while the user has another backup in progress. In this case, as the another backup is in progress a lock mechanism will be applied to it and blocks the deletion of the backup.
 create vol dak and attach to the same node vol bak is attached connect to node via ssh and issue dd if=/dev/urandom of=/dev/longhorn/dak status=progress wait for a bunch of data to be written (1GB) take a backup(1) wait for a bunch of data to be written (1GB) take a backup(2) immediately request deletion of backup(1) verify that backup(2) completes successfully.</description>
    </item>
    
    <item>
      <title>[#1341](https://github.com/longhorn/longhorn/issues/1341) concurrent backup test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/concurrent-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/concurrent-backup/</guid>
      <description> Take a manual backup of the volume bak while a recurring backup is running verify that backup got created verify that backup sticks around even when recurring backups are cleaned up  </description>
    </item>
    
    <item>
      <title>[#1355](https://github.com/longhorn/longhorn/issues/1355) The node the restore volume attached to is down</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/restore-volume-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/restore-volume-node-down/</guid>
      <description> Create a backup. Create a restore volume from the backup. Power off the volume attached node during the restoring. Wait for the Longhorn node down. Wait for the restore volume being reattached and starting restoring volume with state Degraded. Wait for the restore complete. Attach the volume and verify the restored data. Verify the volume works fine.  </description>
    </item>
    
    <item>
      <title>[#1366](https://github.com/longhorn/longhorn/issues/1366) &amp;&amp; [#1328](https://github.com/longhorn/longhorn/issues/1328) The node the DR volume attached to is rebooted</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/dr-volume-node-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/dr-volume-node-rebooted/</guid>
      <description>Scenario 1  Create a pod with Longhorn volume. Write data to the volume and get the md5sum. Create the 1st backup for the volume. Create a DR volume from the backup. Wait for the DR volume starting the initial restore. Then reboot the DR volume attached node immediately. Wait for the DR volume detached then reattached. Wait for the DR volume restore complete after the reattachment. Activate the DR volume and check the data md5sum.</description>
    </item>
    
    <item>
      <title>[#1404](https://github.com/longhorn/longhorn/issues/1404) test backup functionality on google cloud and other s3 interop providers.</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/google-cloud-s3-interop-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/google-cloud-s3-interop-backups/</guid>
      <description> create vol s3-testand mount to a node on /mnt/s3-test via pvc write some data on vol s3-test take backup(1) write new data on vol s3-test take backup(2) restore backup(1) verify data is consistent with backup(1) restore backup(2) verify data is consistent with backup(2) delete backup(1) delete backup(2) delete backup volume s3-test verify volume path is removed  </description>
    </item>
    
    <item>
      <title>[#1431](https://github.com/longhorn/longhorn/issues/1431) backup block deletion test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/backup-block-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/backup-block-deletion/</guid>
      <description>create vol blkand mount to a node on /mnt/blk take backup(1) dd if=/dev/urandom of=/mnt/blk/data2 bs=2097152 count=10 status=progress take backup(2) dd if=/dev/urandom of=/mnt/blk/data3 bs=2097152 count=10 status=progress take backup(3) diff backup(2) backup(3) (run through json beautifier for easier comparison) delete backup(2) verify that the blocks solely used by backup(2) are deleted verify that the shared blocks between backup(2) and backup(3) are retained delete backup(3) wait delete backup(1) wait verify no more blocks verify volume.</description>
    </item>
    
    <item>
      <title>[#2206](https://github.com/longhorn/longhorn/issues/2206) Fix the spinning disk on Longhorn</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/simulated-slow-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/simulated-slow-disk/</guid>
      <description>This case requires the creation of a slow virtual disk with dmsetup.
  Make a slow disk:
 Make a disk image file: truncate -s 10g slow.img Create a loopback device: losetup --show -P -f slow.img Get the block size of the loopback device: blockdev --getsize /dev/loopX Create slow device: echo &amp;quot;0 &amp;lt;blocksize&amp;gt; delay /dev/loopX 0 500&amp;quot; | dmsetup create dm-slow Format slow device: mkfs.ext4 /dev/mapper/dm-slow Mount slow device: mount /dev/mapper/dm-slow /mnt    Build longhorn-engine and run it on the slow disk.</description>
    </item>
    
    <item>
      <title>Air gap installation</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/air-gap-installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/air-gap-installation/</guid>
      <description>Need to test air gap installation manually for now.</description>
    </item>
    
    <item>
      <title>Air gap installation with an instance-manager-image name longer than 63 characters</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/air-gap-instance-manager-name/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/air-gap-instance-manager-name/</guid>
      <description>Host instance manager image under a name more than 63 characters in Docker hub Update longhorn-manager deployment flag &amp;ndash;instance-manager-image to that value Try to create a new volume and attach it.  Expected behavior:There should be no error.</description>
    </item>
    
    <item>
      <title>Automatically Upgrading Longhorn Engine Test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/auto-upgrade-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/auto-upgrade-engine/</guid>
      <description>Longhorn version = 1.1.1  Reference ticket 2152
Test basic upgrade  Install old Longhorn version. E.g., &amp;lt;= v1.0.2 Create a volume, attach it to a pod, write some data. Create a DR volume and leave it in the detached state. Upgrade to Longhorn master Set setting concurrent automatic engine upgrade per node limit to 3 Verify that volumes&#39; engines are upgraded automatically.  Test concurrent upgrade  Create a StatefulSet of scale 10 using 10 Longhorn volume.</description>
    </item>
    
    <item>
      <title>Backing Image Error Reporting and Retry</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/backing-image-error-reporting-and-retry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/backing-image-error-reporting-and-retry/</guid>
      <description>Backing image with an invalid URL schema  Create a backing image via a invalid download URL. e.g., httpsinvalid://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2, https://longhorn-backing-image.s3-us-west-1.amazonaws.invalid.com/parrot.raw. Wait for the download start. The backing image data source pod, which is used to download the file from the URL, should become Failed then be cleaned up immediately. The corresponding and only entry in the disk file status should be failed. The error message in this entry should explain why the downloading or the pod becomes failed.</description>
    </item>
    
    <item>
      <title>Backing Image on a down node</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/backing-image-on-a-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/backing-image-on-a-down-node/</guid>
      <description>Update the settings:  Disable Node Soft Anti-affinity. Set Replica Replenishment Wait Interval to a relatively long value.   Create a backing image. Wait for the backing image being ready in the 1st disk. Create 2 volumes with the backing image and attach them on different nodes. Verify:  the disk state map of the backing image contains the disks of all replicas, and the state is running for all disks.</description>
    </item>
    
    <item>
      <title>BestEffort Recurring Job Cleanup</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/besteffort-recurring-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/besteffort-recurring-job/</guid>
      <description>Set up a BackupStore anywhere (since the cleanup fails at the Engine level, any BackupStore can be used. Add both of the Engine Images listed here:   quay.io/ttpcodes/longhorn-engine:no-cleanup - Snapshot and Backup deletion are both set to return an error. If the Snapshot part of a Backup fails, that will error out first and Backup deletion will not be reached. quay.io/ttpcodes/longhorn-engine:no-cleanup-backup - Only Backup deletion is set to return an error.</description>
    </item>
    
    <item>
      <title>Change imagePullPolicy to IfNotPresent Test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/change-imagepullpolicy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/change-imagepullpolicy/</guid>
      <description> Install Longhorn using Helm chart with the new longhorn master Verify that Engine Image daemonset, Manager daemonset, UI deployment, Driver Deployer deployment has the field spec.template.spec.containers.imagePullPolicy set to IfNotPresent run the bash script dev/scripts/update-image-pull-policy.sh inside longhorn repo Verify that Engine Image daemonset, Manager daemonset, UI deployment, Driver Deployer deployment has the field spec.template.spec.containers.imagePullPolicy set back to Always  </description>
    </item>
    
    <item>
      <title>Cluster using customize kubelet root directory</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/cluster-using-customized-kubelet-root-directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/cluster-using-customized-kubelet-root-directory/</guid>
      <description> Set up a cluster using a customized kubelet root directory. e.g., launching k3s k3s server --kubelet-arg &amp;quot;root-dir=/var/lib/longhorn-test&amp;quot; &amp;amp; Install Longhorn with env KUBELET_ROOT_DIR in longhorn-driver-deployer being set to the corresponding value. Launch a pod using Longhorn volumes via StorageClass. Everything should work fine. Delete the pod and the PVC. Everything should be cleaned up.  </description>
    </item>
    
    <item>
      <title>Compatibility with k3s and SELinux</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/k3s-selinux-compatibility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/k3s-selinux-compatibility/</guid>
      <description>Set up a node with CentOS and make sure that the output of sestatus indicates that SELinux is enabled and set to Enforcing. Run the k3s installation script. Install Longhorn. The system should come up successfully. The logs of the Engine Image pod should only say installed, and the system should be able to deploy a Volume successfully from the UI.  Note: There appears to be some problems with running k3s on CentOS, presumably due to the firewalld rules.</description>
    </item>
    
    <item>
      <title>CSI Sanity Check</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/csi-sanity-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/csi-sanity-check/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2076
Run csi-sanity   Prepare Longhorn cluster and setup backup target.
  Make csi-sanity binary from csi-test.
  On one of the cluster node, run csi-sanity binary.
csi-sanity -csi.endpoint /var/lib/kubelet/obsoleted-longhorn-plugins/driver.longhorn.io/csi.sock -ginkgo.skip=&amp;#34;should create volume from an existing source snapshot|should return appropriate values|should succeed when creating snapshot with maximum-length name|should succeed when requesting to create a snapshot with already existing name and same source volume ID|should fail when requesting to create a snapshot with already existing name and different source volume ID&amp;#34;  NOTE</description>
    </item>
    
    <item>
      <title>Degraded availability with added nodes</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/degraded-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/degraded-availability/</guid>
      <description>https://github.com/longhorn/longhorn/issues/1701
Volume creation using UI with degraded availability and added node Start with one node cluster.
 Create a Deployment Pod with a volume and three replicas.  After the volume is attached, scheduling error should be seen from the UI. But no functional impact.   Write data to the Pod. Scale down the deployment to 0 to detach the volume.  scheduling error should be gone.   Scale up the deployment back to 1 verify the data.</description>
    </item>
    
    <item>
      <title>Disk migration in AWS ASG</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/disk-migration-in-aws-asg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/disk-migration-in-aws-asg/</guid>
      <description>Some Longhorn worker nodes in AWS Auto Scaling group is in replacement  Launch a Kubernetes cluster with the nodes in AWS Auto Scaling group. Make sure there is an additional EBS attached to instance with setting Delete on Termination disabled. Deploy Longhorn v1.1.0 on the cluster and Set ReplicaReplenishmentWaitInterval. Make sure it&amp;rsquo;s longer than the time needs for node replacement. Deploy some workloads using Longhorn volumes. Trigger the ASG instance refresh in AWS.</description>
    </item>
    
    <item>
      <title>DR volume related latest backup deletion test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/</guid>
      <description>DR volume keeps getting the latest update from the related backups. Edge cases where the latest backup is deleted can be test as below.
Case 1:  Create a volume and take multiple backups for the same. Delete the latest backup. Create another cluster and set the same backup store to access the backups created in step 1. Go to backup page and click on the backup. Verify the Create Disaster Recovery option is enabled for it.</description>
    </item>
    
    <item>
      <title>Improve Node Failure Handling By Automatically Force Delete Terminating Pods of StatefulSet/Deployment On Downed Node</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/improve-node-failure-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/improve-node-failure-handling/</guid>
      <description>Setup a cluster of 3 worker nodes Install Longhorn and set Default Replica Count = 2 (because we will turn off one node) Create a StatefulSet with 2 pods using the command: kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/statefulset.yaml  Create a volume + pv + pvc named vol1 and create a deployment(1 pod) of default ubuntu named shell with the usage of pvc vol1 mounted under /mnt/vol1 Find the node which contains one pod of the StatefulSet/Deployment.</description>
    </item>
    
    <item>
      <title>Kubernetes upgrade test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/kubernetes-upgrade-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/kubernetes-upgrade-test/</guid>
      <description>We also need to cover the Kubernetes upgrade process for supported Kubernetes version, make sure pod and volumes works after a major version upgrade.
Related Issue https://github.com/longhorn/longhorn/issues/2566
Test with K8s upgrade  Create a K8s (Immediate prior version) cluster with 3 worker nodes and 1 control plane. Deploy Longhorn version (Immediate prior version) on the cluster. Create a volume and attach to a pod. Write data to the volume and compute the checksum.</description>
    </item>
    
    <item>
      <title>Longhorn installation multiple times</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/stability/multiple-installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/stability/multiple-installation/</guid>
      <description>Create a cluster(3 worker nodes and 1 etc/control plane). Deploy the longhorn app. Once longhorn deployed successfully, uninstall longhorn. Repeat the steps 2 and 3 multiple times. Run the below script to install and uninstall longhorn continuously for some time.  installcount=0 while true; echo `date` do kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml pod=`kubectl get pods -n longhorn-system | grep -i &#39;longhorn-manager&#39; | grep -i &#39;running&#39; | awk -F &#39; &#39; &#39;{print $2}&#39; | grep &#39;1/1&#39; | wc -l` count=0 while [ $pod !</description>
    </item>
    
    <item>
      <title>Longhorn system upgrade with restore</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/system-upgrade-with-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/system-upgrade-with-restore/</guid>
      <description>Notice that the behaviors will be different if the cluster node roles are different. e.g., A cluster contains 1 dedicated master node + 3 worker node is different from a cluster contains 3 nodes which are both master and worker. This test may need to be validated for both kind of cluster.
 Install Longhorn system. Deploy one more compatible engine image. Deploy some workloads using Longhorn volumes then write some data.</description>
    </item>
    
    <item>
      <title>Longhorn Upgrade test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/longhorn-upgrade-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/longhorn-upgrade-test/</guid>
      <description>Setup  2 attached volumes with data. 2 detached volumes with data. 2 new volumes without data. 2 deployments of one pod. 1 statefulset of 10 pods. Auto Salvage set to disable.  Test After upgrade:
 Make sure the existing instance managers didn&amp;rsquo;t restart. Make sure pods didn&amp;rsquo;t restart. Check the contents of the volumes. If the Engine API version is incompatible, manager cannot do anything about the attached volumes except detaching it.</description>
    </item>
    
    <item>
      <title>Longhorn with engine is not deployed on all the nodes</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/partial-engine-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/partial-engine-deployment/</guid>
      <description>Related Issue https://github.com/longhorn/longhorn/issues/2081
Scenarios: Case 1: Test volume operations when some of the engine image DaemonSet pods are miss scheduled  Install Longhorn in a 3-node cluster: node-1, node-2, node-3 Create a volume, vol-1, of 3 replicas Create another volume, vol-2, of 3 replicas Taint node-1 with the taint: key=value:NoSchedule Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for vol-1  Case 2: Test volume operations when some of the engine image DaemonSet pods are not fully deployed  Continue from case 1 Attach vol-1 to node-1.</description>
    </item>
    
    <item>
      <title>Longhorn with engine is not deployed on all the nodes</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/partial-engine-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/partial-engine-deployment/</guid>
      <description>Related Issue https://github.com/longhorn/longhorn/issues/2081
Scenarios: Case 1: Test volume operations when some of the engine image DaemonSet pods are miss scheduled  Install Longhorn in a 3-node cluster: node-1, node-2, node-3 Create a volume, vol-1, of 3 replicas Create another volume, vol-2, of 3 replicas Taint node-1 with the taint: key=value:NoSchedule Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for vol-1  Case 2: Test volume operations when some of engine image DaemonSet pods are not fully deployed  Continue from case 1 Attach vol-1 to node-1.</description>
    </item>
    
    <item>
      <title>New Node with Custom Data Directory</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/new-node-custom-data-directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/new-node-custom-data-directory/</guid>
      <description>Make sure that the default Longhorn setup has all nodes with /var/lib/rancher/longhorn/ as the default Longhorn disk under the Node page. Additionally, check the Setting page and make sure that the &amp;ldquo;Default Data Path&amp;rdquo; setting has been set to /var/lib/rancher/longhorn/ by default. Now, change the &amp;ldquo;Default Data Path&amp;rdquo; setting to something else, such as /home, and save the new settings. Add a new node to the cluster with the proper dependencies to run Longhorn.</description>
    </item>
    
    <item>
      <title>NFSv4 Enforcement (No NFSv3 Fallback)</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/nfsv4-enforcement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/nfsv4-enforcement/</guid>
      <description>Since the client falling back to NFSv3 usually results in a failure to mount the NFS share, the way we can check for NFSv3 fallback is to check the error message returned and see if it mentions rpc.statd, since dependencies on rpc.statd and other services are no longer needed for NFSv4, but are needed for NFSv3. The NFS mount should not fall back to NFSv3 and instead only give the user a warning that the server may be NFSv3:</description>
    </item>
    
    <item>
      <title>Node creation and deletion with restore</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/node-creation-deletion-with-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/node-creation-deletion-with-restore/</guid>
      <description>Notice that the behaviors will be different if the cluster node roles are different. e.g., A cluster contains 1 dedicated master node + 3 worker node is different from a cluster contains 3 nodes which are both master and worker. This test may need to be validated for both kind of cluster.
 Deploy a 3-worker-node cluster then install Longhorn system. Deploy some workloads using Longhorn volumes then write some data.</description>
    </item>
    
    <item>
      <title>Node disconnection test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-disconnection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-disconnection/</guid>
      <description>https://github.com/longhorn/longhorn/issues/1545 For disconnect node : https://github.com/longhorn/longhorn/files/4864127/network_down.sh.zip
Case 1:  Disable the setting auto-salvage. Create and attach a volume. Keep writing data to the volume. Disconnect the node that the volume attached to for 100 seconds during the data writing. Wait for the node back. The volume will be detached then reattached automatically. And there are some replicas still running after the reattachment.  Case 2:  Launch Longhorn. Use statefulset launch a pod with the volume and write some data.</description>
    </item>
    
    <item>
      <title>Node drain and deletion test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-drain-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-drain-deletion/</guid>
      <description>Drain with force Make sure the volumes on the drained/removed node can be detached or recovered correctly. The related issue: https://github.com/longhorn/longhorn/issues/1214
 Deploy a cluster contains 3 worker nodes N1, N2, N3. Deploy Longhorn. Create a 1-replica deployment with a 3-replica Longhorn volume. The volume is attached to N1. Write some data to the volume and get the md5sum. Force drain and remove N2, which contains one replica only. kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data=true --force=true --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;  Wait for the volume Degraded.</description>
    </item>
    
    <item>
      <title>Physical node down</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/physical-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/physical-node-down/</guid>
      <description>One physical node down should result in the state of that node change to Down. When using with CSI driver, one node with controller (StatefulSet/Deployment) and pod down should result in Kubernetes migrate the pod to another node, and Longhorn volume should be able to be used on that node as well. Test scenarios for this are documented here. Reboot the node that the controller attached to. After reboot complete, the volume should be reattached to the node.</description>
    </item>
    
    <item>
      <title>Priority Class Default Setting</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/priorityclass-default-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/priorityclass-default-setting/</guid>
      <description>There are three different cases we need to test when the user inputs a default setting for Priority Class:
 Install Longhorn with no priority-class set in the default settings. The Priority Class setting should be empty after the installation completes according to the longhorn-ui, and the default Priority of all Pods in the longhorn-system namespace should be 0:  ~ kubectl -n longhorn-system describe pods | grep Priority # should be repeated many times Priority: 0 Install Longhorn with a nonexistent priority-class in the default settings.</description>
    </item>
    
    <item>
      <title>Prometheus Support</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/prometheus_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/prometheus_support/</guid>
      <description>Prometheus Support allows user to monitor the longhorn metrics. The details are available at https://longhorn.io/docs/1.1.0/monitoring/
Monitor longhorn  Deploy the Prometheus-operator, ServiceMonitor pointing to longhorn-backend and Prometheus as mentioned in the doc. Create an ingress pointing to Prometheus service. Access the Prometheus web UI using the ingress created in the step 2. Select the metrics from below to monitor the longhorn resources.  longhorn_volume_actual_size_bytes longhorn_volume_capacity_bytes longhorn_volume_robustness longhorn_volume_state longhorn_instance_manager_cpu_requests_millicpu longhorn_instance_manager_cpu_usage_millicpu longhorn_instance_manager_memory_requests_bytes longhorn_instance_manager_memory_usage_bytes longhorn_manager_cpu_usage_millicpu longhorn_manager_memory_usage_bytes longhorn_node_count_total longhorn_node_status longhorn_node_cpu_capacity_millicpu longhorn_node_cpu_usage_millicpu longhorn_node_memory_capacity_bytes longhorn_node_memory_usage_bytes longhorn_node_storage_capacity_bytes longhorn_node_storage_reservation_bytes longhorn_node_storage_usage_bytes longhorn_disk_capacity_bytes longhorn_disk_reservation_bytes longhorn_disk_usage_bytes   Deploy workloads which use Longhorn volumes into the cluster.</description>
    </item>
    
    <item>
      <title>Re-deploy CSI components when their images change</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/update_csi_components_when_images_change/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/update_csi_components_when_images_change/</guid>
      <description> Install Longhorn Change the longhorn-driver-deployer yaml at https://github.com/longhorn/longhorn-manager/blob/c2ceb9f3f991810f811601d8c41c09b67fb50746/deploy/install/02-components/04-driver.yaml#L50 to use the new images for some CSI components Kubectl apply -f the longhorn-driver-deployer yaml Verify that only CSI components with the new images are re-deployed and have new images Redeploy longhorn-driver-deployer without changing the images. Verify that no CSI component is re-deployed  </description>
    </item>
    
    <item>
      <title>Recover Longhorn volumes after Rancher cluster restore</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/recover-volumes-after-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/recover-volumes-after-restore/</guid>
      <description>Notice that the behaviors will be different if the cluster node roles are different. e.g., A cluster contains 1 dedicated master node + 3 worker node is different from a cluster contains 3 nodes which are both master and worker. This test may need to be validated for both kind of cluster.
 Launch a Longhorn system. Deploy one more compatible engine image. Prepare some volumes with workloads:  Create and attach volume A via UI.</description>
    </item>
    
    <item>
      <title>Recurring backup job interruptions</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/</guid>
      <description>Related Issue https://github.com/longhorn/longhorn/issues/1882
Scenario 1- Allow Recurring Job While Volume Is Detached disabled, attached pod scaled down while the recurring backup was in progress.  Create a volume, attach to a pod of a statefulSet, and write 800 Mi data into it. Set a recurring job. While the recurring job is in progress, scale down the pod to 0 of the statefulSet. Volume first detached and cron job gets finished saying unable to complete the backup.</description>
    </item>
    
    <item>
      <title>Replica Rebuilding</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/replica-rebuilding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/replica-rebuilding/</guid>
      <description>Create and attach a volume. Write a large amount of data to the volume. Disable disk scheduling and the node scheduling for one replica. Crash the replica progress. Verify  the corresponding replica will become ERROR. the volume will keep robustness Degraded.   Enable the disk scheduling. Verify nothing changes. Enable the node scheduling. Verify.  the failed replica is reused by Longhorn. the rebuilding progress in UI page looks good.</description>
    </item>
    
    <item>
      <title>Return an error when fail to remount a volume</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/error-fail-remount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/error-fail-remount/</guid>
      <description>Case 1: Volume with a corrupted filesystem try to remount Steps to reproduce bug:
 Create a volume of size 1GB, say terminate-immediatly volume. Create PV/PVC from the volume terminate-immediatly Create a deployment of 1 pod with image ubuntu:xenial and the PVC terminate-immediatly in default namespace Find the node on which the pod is scheduled to. Let&amp;rsquo;s say the node is Node-1 ssh into Node-1 destroy the filesystem of terminate-immediatly by running command dd if=/dev/zero of=/dev/longhorn/terminate-immediatly Find and kill the engine instance manager in Node-X.</description>
    </item>
    
    <item>
      <title>Reusing failed replica for rebuilding</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/</guid>
      <description>Longhorn upgrade with node down and removal  Launch Longhorn v1.0.x Create and attach a volume, then write data to the volume. Directly remove a Kubernetes node, and shut down a node. Wait for the related replicas failure. Then record replica.Spec.DiskID for the failed replicas. Upgrade to Longhorn master Verify the Longhorn node related to the removed node is gone. Verify  replica.Spec.DiskID on the down node is updated and the field of the replica on the gone node is unchanged.</description>
    </item>
    
    <item>
      <title>Set Tolerations/PriorityClass For System Components</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2120
Manual Tests:
Case 1: Existing Longhorn installation  Install Longhorn master. Change toleration in UI setting Verify that longhorn.io/last-applied-tolerations annotation and toleration of manager, drive deployer, UI are not changed. Verify that longhorn.io/last-applied-tolerations annotation and toleration for managed components (CSI components, IM pods, share manager pod, EI daemonset, backing-image-manager, cronjob) are updated correctly  Case 2: New installation by Helm  Install Longhorn master, set tolerations like:  defaultSettings: taintToleration: &amp;#34;key=value:NoSchedule&amp;#34; longhornManager: priorityClass: ~ tolerations: - key: key operator: Equal value: value effect: NoSchedule longhornDriver: priorityClass: ~ tolerations: - key: key operator: Equal value: value effect: NoSchedule longhornUI: priorityClass: ~ tolerations: - key: key operator: Equal value: value effect: NoSchedule  Verify that the toleration is added for: IM pods, Share Manager pods, CSI deployments, CSI daemonset, the backup jobs, manager, drive deployer, UI Uninstall the Helm release.</description>
    </item>
    
    <item>
      <title>Single replica node down</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/single-replica-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/single-replica-node-down/</guid>
      <description>Related Issues https://github.com/longhorn/longhorn/issues/2329 https://github.com/longhorn/longhorn/issues/2309
Default Setting Automatic salvage is enabled.
Node restart/down scenario with Pod Deletion Policy When Node is Down set to default value do-nothing.  Create RWO|RWX volume with replica count = 1 &amp;amp; data locality = enabled|disabled. Create deployment|statefulset for volume. Power down node of volume/replica. The workload pod will get stuck in the unknown state. Volume will fail to attach since volume is not ready (i.e remains faulted, since single replica is on downed node).</description>
    </item>
    
    <item>
      <title>Snapshot while writing data in the volume</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/basic-operations-parallelism/snapshot-while-writing-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/basic-operations-parallelism/snapshot-while-writing-data/</guid>
      <description>Related issue: https://github.com/longhorn/longhorn/issues/2187
Scenario  Create a kubernetes pod + pvc that mounts a Longhorn volume. Write 5 Gib into the pod using dd if=/dev/urandom of=/mnt/&amp;lt;volume&amp;gt; count=5000 bs=1M conv=fsync status=progress While running the above command initiate a snapshot. Verify the logs of the instance-manager using kubetail instance-manager -n longhorn-system. There should some logs related to freezing and unfreezing the filesystem. Like Froze filesystem of volume mounted ... Verify snapshot succeeded and dd operation will complete.</description>
    </item>
    
    <item>
      <title>Support Kubelet Volume Metrics</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/kubelet_volume_metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/kubelet_volume_metrics/</guid>
      <description>Intro Kubelet exposes kubelet_volume_stats_* metrics. Those metrics measure PVC&amp;rsquo;s filesystem related information inside a Longhorn block device.
Test steps:  Create a cluster and set up this monitoring system: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack Install Longhorn. Deploy some workloads using Longhorn volumes. Make sure there are some workloads using Longhorn PVCs in volumeMode: Block and some workloads using Longhorn PVCs in volumeMode: Filesystem. See https://longhorn.io/docs/1.0.2/references/examples/ for examples. Create ingress to Prometheus server and Grafana. Navigate to Prometheus server, verify that all Longhorn PVCs in volumeMode: Filesystem show up in metrics: kubelet_volume_stats_capacity_bytes kubelet_volume_stats_available_bytes kubelet_volume_stats_used_bytes kubelet_volume_stats_inodes kubelet_volume_stats_inodes_free kubelet_volume_stats_inodes_used.</description>
    </item>
    
    <item>
      <title>Test access style for S3 compatible backupstore</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/test-s3-access-style/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/test-s3-access-style/</guid>
      <description>Case 1: Using Alibaba Cloud OSS bucket as backupstore  Create an OSS bucket within Region China in Alibaba Cloud(Aliyun). Create a secret without VIRTUAL_HOSTED_STYLE for the OSS bucket. Set backup target and the secret in Longhorn UI. Try to list backup. Then the error error: AWS Error: SecondLevelDomainForbidden Please use virtual hosted style to access. is triggered. Add VIRTUAL_HOSTED_STYLE: dHJ1ZQ== # true to the secret. Backup list/create/delete/restore work fine after the configuration.</description>
    </item>
    
    <item>
      <title>Test Additional Printer Columns</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/additional-printer-columns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/additional-printer-columns/</guid>
      <description>For each of the case below:
 Fresh installation of Longhorn. (make sure to delete all Longhorn CRDs before installation) Upgrade from older version.  Run:
kubectl get &amp;lt;LONGHORN-CRD&amp;gt; -n longhorn-system Verify that the output contains information as specify in the additionalPrinerColumns at here</description>
    </item>
    
    <item>
      <title>Test backing image</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test-backing-image-upload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test-backing-image-upload/</guid>
      <description>Test upload  Prepare a large backing image file (make sure the size is greater than 1Gi and the uploading time is longer than 1 minute) in local. Click the backing image creation button in UI, choose Upload From Local, select the file then start upload. Wait for the initialization complete. Then the upload progress will be shown. During the uploading, verify the corresponding backing image data source pod won&amp;rsquo;t use too many CPU (50 ~ 200m) and memory(50 ~ 200Mi) resources.</description>
    </item>
    
    <item>
      <title>Test Backing Image during Longhorn upgrade</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/backing-image-during-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/backing-image-during-upgrade/</guid>
      <description>System upgrade with compatible backing image manager image  Deploy Longhorn. Then set Concurrent Automatic Engine Upgrade Per Node Limit to a positive value to enable volume engine auto upgrade. Create 2 backing images: a large one and a small one. Longhorn will start preparing the 1st file for both backing image immediately via launching backing image data source pods. Wait for the small backing image being ready in the 1st disk.</description>
    </item>
    
    <item>
      <title>Test Backup Creation With Old Engine Image</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/backup-creation-with-old-engine-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/backup-creation-with-old-engine-image/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2897
Test Step Given with Longhorn v1.2.0-rc2 or above. And deploy engine image oldEI older than v1.2.0 (for example: longhornio/longhorn-engine:v1.1.2). And create volume vol-old-engine. And attach volume vol-old-engine to one of a node. And upgrade volume vol-old-engine to engine image oldEI.
When create backup of volume vol-old-engine.
Then watch kubectl kubectl get backups.longhorn.io -l backup-volume=vol-old-engine -w. And should see two backups temporarily (in transition state). And should see only one backup be left after a while.</description>
    </item>
    
    <item>
      <title>Test backup listing S3/NFS</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/stress/backup-listing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/stress/backup-listing/</guid>
      <description>1. Backup listing with more than 1000 backups - S3  Deploy Longhorn on a kubernetes cluster. Set up S3 backupStore. Create a volume of 2Gi and attach it to a pod. Write some data into it and compute md5sum. Open browser developer tool. Create one backup by clicking LH GUI (It&amp;rsquo;ll call snapshotCreate and snapshotBackup APIs). Copy the snapshotBackup API call, right click Copy -&amp;gt; Copy as cURL. Run the curl command over 1k times in the Shell.</description>
    </item>
    
    <item>
      <title>Test CronJob For Volumes That Are Detached For A Long Time</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/delete-cronjob-for-detached-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/delete-cronjob-for-detached-volumes/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2513
Steps  Make sure the setting Allow Recurring Job While Volume Is Detached is disabled Create a volume. Attach to a node. Create a recurring backup job that run every minute. Wait for the cronjob to be scheduled a few times. Detach the volume. Verify that the CronJob get deleted. Wait 2 hours (&amp;gt; 100 mins). Attach the volume to a node. Verify that the CronJob get created.</description>
    </item>
    
    <item>
      <title>Test Disable IPv6</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/disable_ipv6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/disable_ipv6/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2136
https://github.com/longhorn/longhorn/issues/2197
Longhorn v1.1.1 should work with IPv6 disabled.
Scenario  Install Kubernetes Disable IPv6 on all the worker nodes using the following Go to the folder /etc/default In the grub file, edit the value GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;ipv6.disable=1&amp;quot; Once the file is saved update by the command update-grub Reboot the node and once the node becomes active, Use the command cat /proc/cmdline to verify &amp;quot;ipv6.disable=1&amp;quot; is reflected in the values  Deploy Longhorn and test basic use cases.</description>
    </item>
    
    <item>
      <title>Test Engine Crash During Live Upgrade</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/engine-crash-during-live-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/engine-crash-during-live-upgrade/</guid>
      <description> Create and attach a volume. Deploy an extra engine image. Send live upgrade request then immediately delete the related engine manager pod/engine process (The new replicas are not in active in this case). Verify the volume will detach then reattach automatically. Verify the upgrade is done during the reattachment. (It actually becomes offline upgrade.)  </description>
    </item>
    
    <item>
      <title>Test File Sync Cancellation</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-file-sync-cancellation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-file-sync-cancellation/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2416
Test step  For test convenience, manually launch the backing image manager pods:  apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: backing-image-manager name: backing-image-manager namespace: longhorn-system spec: selector: matchLabels: app: backing-image-manager template: metadata: labels: app: backing-image-manager spec: containers: - name: backing-image-manager image: longhornio/backing-image-manager:master imagePullPolicy: Always securityContext: privileged: true command: - backing-image-manager - --debug - daemon - --listen - 0.0.0.0:8000 readinessProbe: tcpSocket: port: 8000 volumeMounts: - name: disk-path mountPath: /data volumes: - name: disk-path hostPath: path: /var/lib/longhorn/ serviceAccountName: longhorn-service-account Download a backing image in the first pod:  # alias bm=&amp;quot;backing-image-manager backing-image&amp;quot; # bm pull --name bi-test --uuid uuid-bi-test --download-url https://cloud-images.</description>
    </item>
    
    <item>
      <title>Test Frontend Traffic</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/ws-traffic-flood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/ws-traffic-flood/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2372
Test Frontend Traffic Given 100 pvc created.
And all pvcs deployed and detached.
When monitor traffic in frontend pod with nload.
apk add nload nload Then should not see a continuing large amount of traffic when there is no operation happening. The smaller spikes are mostly coming from event resources which possibly could be enhanced later (https://github.com/longhorn/longhorn/issues/2433).</description>
    </item>
    
    <item>
      <title>Test Frontend Web-socket Data Transfer When Resource Not Updated</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/full-ws-data-tranfer-when-no-updates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/full-ws-data-tranfer-when-no-updates/</guid>
      <description>Related issue https://github.com/longhorn/longhorn-manager/pull/918 https://github.com/longhorn/longhorn/issues/2646 https://github.com/longhorn/longhorn/issues/2591
Test Data Send Over Web-socket When No Resource Updated Given 1 PVC/Pod created. And the Pod is not writing to the mounted volume.
When monitor network traffic with browser inspect tool.
Then wait for 3 mins And should not see data send over web-socket when there are no updates to the resources.
Test Data Send Over Web-socket Resource Updated Given monitor network traffic with browser inspect tool.</description>
    </item>
    
    <item>
      <title>Test instance manager cleanup during uninstall</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test-instance-manager-cleanup-during-uninstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test-instance-manager-cleanup-during-uninstall/</guid>
      <description>Deploy Longhorn v1.1.2 Launch some running volumes. Upgrade to v1.2.0. ==&amp;gt; All old unused engine managers should be cleaned up automatically. Make sure all running volumes keep state running. Upgrade all volumes. ==&amp;gt; All old replica managers should be cleaned up automatically. Detach all running volumes. ==&amp;gt; All old engine managers should be cleaned up automatically. do offline upgrade then reattach these volumes. Directly uninstall the Longhorn system. And use kubectl -n longhorn-system get lhim -w to verify that the system doesn&amp;rsquo;t loop in instance manager cleanup-recreation.</description>
    </item>
    
    <item>
      <title>Test Instance Manager IP Sync</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/instance-manager-ip-sync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/instance-manager-ip-sync/</guid>
      <description>Test step:  Launch longhorn system Create and attach a volume Follow this doc to manually modify the IP of one instance-manager-r. e.g., curl -k -XPATCH -H &amp;quot;Accept: application/json&amp;quot; -H &amp;quot;Content-Type: application/merge-patch+json&amp;quot; -H &amp;quot;Authorization: Bearer kubeconfig-xxxxxx&amp;quot; --data &#39;{&amp;quot;status&amp;quot;:{&amp;quot;ip&amp;quot;:&amp;quot;1.1.1.1&amp;quot;}}&#39; https://172.104.72.64/k8s/clusters/c-znrxc/apis/longhorn.io/v1beta1/namespaces/longhorn-system/instancemanagers/instance-manager-r-63ece607/status  Notice that the bearer token kubeconfig-xxx can be found in your kube config file Remember to add /status at the end of the URL   Verify the IP of the instance manager still matches the pod IP Verify the volume can be detached.</description>
    </item>
    
    <item>
      <title>Test Instance Manager Streaming Connection Recovery</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/instance-manager-streaming-connection-recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/instance-manager-streaming-connection-recovery/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2561
Test Step Given A cluster with Longhorn
And create a volume and attach it to a pod.
And exec into a longhorn manager pod and kill the connection with an engine or replica instance manager pod. The connections are instance manager pods&#39; IP with port 8500.
$ kl exec -it longhorn-manager-5z8zn -- bash root@longhorn-manager-5z8zn:/# ss Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port tcp ESTAB 0 0 10.</description>
    </item>
    
    <item>
      <title>Test ISCSI Installation on EKS</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/iscsi_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/iscsi_installation/</guid>
      <description>This is for EKS or similar users who doesn&amp;rsquo;t need to log into each host to install &amp;lsquo;ISCSI&amp;rsquo; individually.
Test steps:
 Create an EKS cluster with 3 nodes. Run the following command to install iscsi on every nodes.  kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/iscsi/longhorn-iscsi-installation.yaml In Longhorn Manager Repo Directory run:  kubectl apply -Rf ./deploy/install/ Longhorn should be able installed successfully. Try to create a pod with a pvc:  kubectl apply -f https://raw.</description>
    </item>
    
    <item>
      <title>Test kubelet restart on a node of the cluster</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/kubelet-restart-on-a-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/kubelet-restart-on-a-node/</guid>
      <description>Related issues: https://github.com/longhorn/longhorn/issues/2629
Case 1: Kubelet restart on RKE1 multi node cluster:  Create a RKE1 cluster with config of 1 etcd/control plane and 3 worker nodes. Deploy Longhorn on the cluster. Deploy prometheus monitoring app on the cluster which is using Longhorn storage class or deploy a statefulSet with Longhorn volume. Write some data into the mount point and compute the md5sum. Restart the kubelet on the node where the statefulSet or Prometheus pod is running using the command sudo docker restart kubelet Observe the volume.</description>
    </item>
    
    <item>
      <title>Test Label-driven Recurring Job</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/label-driven-recurring-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/label-driven-recurring-job/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/467
Test Recurring Job Concurrency Given create snapshot recurring job with concurrency set to 2 and include snapshot recurring job default in groups.
When create volume test-job-1.
And create volume test-job-2.
And create volume test-job-3.
And create volume test-job-4.
And create volume test-job-5.
Then moniter the cron job pod log.
And should see 2 jobs created concurrently.
When update snapshot1 recurring job with concurrency set to 3.
Then moniter the cron job pod log.</description>
    </item>
    
    <item>
      <title>Test Longhorn Deployment on RKE2 with CIS-1.5 profile</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/rke2-cis-1.5-profile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/rke2-cis-1.5-profile/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2292
Longhorn v1.1.1 should work on RKE2 with CIS-1.5 profile
Scenario  Prepare 1 control plane node and 3 worker nodes Install RKE2 with CIS-1.5 profile on 1 control plane node  sudo su - systemctl disable firewalld systemctl stop firewalld curl -sfL https://get.rke2.io | sh - systemctl enable rke2-server.service cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/sysctl.d/60-rke2-cis.conf vm.panic_on_oom=0 vm.overcommit_memory=1 kernel.panic=10 kernel.panic_on_oops=1 EOF sudo systemctl restart systemd-sysctl useradd -r -c &amp;#34;etcd user&amp;#34; -s /sbin/nologin -M etcd mkdir -p /etc/rancher/rke2/ cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/rancher/rke2/config.</description>
    </item>
    
    <item>
      <title>Test Node Delete</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/delete-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/delete-node/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2186 https://github.com/longhorn/longhorn/issues/2462
Delete Method Should verify with both of the delete methods.
 Bulk Delete - This is the Delete on the Node page. Node Delete - This is the Remove Node for each node Operation drop-down list.  Test Node Delete - should grey out when node not down Given node not Down.
When Try to delete any node.
Then Should see button greyed out.
Test Node Delete Given pod with pvc created.</description>
    </item>
    
    <item>
      <title>Test node deletion</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-deletion/</guid>
      <description>Case 1: Delete multiple kinds of nodes:  Deploy Longhorn. Shut down the VM for one node and wait for the node Down. Disable another node. Delete the above 2 nodes. Make sure the corresponding Kubernetes node object is deleted. &amp;ndash;&amp;gt; The related Longhorn node objects will be cleaned up immediately, too. Add new nodes with the same names for the cluster. &amp;ndash;&amp;gt; The new nodes are available.  Case 2: Delete nodes when there are running volumes:  Deploy Longhorn.</description>
    </item>
    
    <item>
      <title>Test Node Selector</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-node-selector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-node-selector/</guid>
      <description>Prepare the cluster  Using Rancher RKE to create a cluster of 2 Windows worker nodes and 3 Linux worker nodes. Rancher will add the taint cattle.io/os=linux:NoSchedule to Linux nodes Kubernetes will add label kubernetes.io/os:linux to Linux nodes  Test steps Repeat the following steps for each type of Longhorn installation: Rancher, Helm, Kubectl:
 Follow the Longhorn document at the PR https://github.com/longhorn/website/pull/287 to install Longhorn with toleration cattle.io/os=linux:NoSchedule and node selector kubernetes.</description>
    </item>
    
    <item>
      <title>Test Read Write Many Feature</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/rwx_feature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/rwx_feature/</guid>
      <description>Prerequisite:  Set up a Cluster of 4 nodes (1 etc/control plane and 3 workers) Deploy Latest Longhorn-master  Create StatefulSet/Deployment with single pod with volume attached in RWX mode.  Create a PVC with RWX mode using longhorn class by selecting the option read write many. Attach the PVC to a StatefulSet/Deployment with 1 pod. Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created. Verify share-manager pod come up healthy.</description>
    </item>
    
    <item>
      <title>Test RWX share-mount ownership reset</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2357
Test RWX share-mount ownership Given Setup one of cluster node to use host FQDN.
root@ip-172-30-0-139:/home/ubuntu# cat /etc/hosts 127.0.0.1 localhost 54.255.224.72 ip-172-30-0-139.lan ip-172-30-0-139 root@ip-172-30-0-139:/home/ubuntu# hostname ip-172-30-0-139 root@ip-172-30-0-139:/home/ubuntu# hostname -f ip-172-30-0-139.lan And Domain = localdomain is commented out in /etc/idmapd.conf on cluster hosts. This is to ensure localdomain is not enforce to sync between server and client. Ref: https://github.com/longhorn/website/pull/279
root@ip-172-30-0-139:~# cat /etc/idmapd.conf [General] Verbosity = 0 Pipefs-Directory = /run/rpc_pipefs # set your own domain here, if it differs from FQDN minus hostname # Domain = localdomain [Mapping] Nobody-User = nobody Nobody-Group = nogroup And pod with rwx pvc deployed to the node with host FQDN.</description>
    </item>
    
    <item>
      <title>Test S3 backupstore in a cluster sitting behind a Http Proxy</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/test-s3-backupstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/test-s3-backupstore/</guid>
      <description>Create a new instance on Linode and setup an Http Proxy server on the instance as in this instruction (you will have to log in to see the instruction) Create a cluster using Rancher as below:  Choose AWS EC2 t2.medium as the node template. The reason to chose EC2 is that its security group makes our lives easier to block the outgoing traffic from the instance and all k8s Pods running inside the instance.</description>
    </item>
    
    <item>
      <title>Test Service Account mount on host</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-service-account-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-service-account-mount/</guid>
      <description>This test case should be tested on both yaml installation, chart installation (Helm and Rancher UI), as well as upgrade scenarios After install Longhorn using on of the above method, ssh into a worker node that has a longhorn-manager pod running check the mount point /run/secrets/kubernetes.io/serviceaccount by running: root@node-1:~# findmnt /run/secrets/kubernetes.io/serviceaccount  Verify that there is no such mount point Kill the longhorn-manager pod on the above node and wait for it to be recreated and running check the mount point /run/secrets/kubernetes.</description>
    </item>
    
    <item>
      <title>Test Snapshot Purge Error Handling</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1895
Longhorn v1.1.1 handles the error during snapshot purge better and reports to Longhorn-manager.
Scenario-1  Create a volume with 3 replicas and attach to a pod. Write some data into the volume and take a snapshot. Delete a replica that will result in creating a system generated snapshot. Wait for replica to finish and take another snapshot. ssh into a node and resize the latest snapshot. (e.g dd if=/dev/urandom count=50 bs=1M of=&amp;lt;SNAPSHOT-NAME&amp;gt;.</description>
    </item>
    
    <item>
      <title>Test System Upgrade with New Instance Manager</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/upgrade-with-new-instance-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/upgrade-with-new-instance-manager/</guid>
      <description>Prepare 3 sets of longhorn-manager and longhorn-instance-manager images. Deploy Longhorn with the 1st set of images. Set Guaranteed Engine Manager CPU and Guaranteed Replica Manager CPU to 15 and 24, respectively. Then wait for the instance manager recreation. Create and attach a volume to a node (node1). Upgrade the Longhorn system with the 2nd set of images. Verify the CPU requests in the pods of both instance managers match the settings.</description>
    </item>
    
    <item>
      <title>Test system upgrade with the deprecated CPU setting</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2207
Test step  Deploy a cluster that each node has different CPUs. Launch Longhorn v1.1.0. Deploy some workloads using Longhorn volumes. Upgrade to the latest Longhorn version. Validate:  all workloads work fine and no instance manager pod crash during the upgrade. The fields node.Spec.EngineManagerCPURequest and node.Spec.ReplicaManagerCPURequest of each node are the same as the setting Guaranteed Engine CPU value in the old version * 1000. The old setting Guaranteed Engine CPU is deprecated with an empty value.</description>
    </item>
    
    <item>
      <title>Test timeout on loss of network connectivity</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/timeout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/timeout/</guid>
      <description>Ping Timeout  Create a docker network:  docker network create -d bridge --subnet 192.168.22.0/24 longhorn-network Start a replica:  docker run --net longhorn-network --ip 192.168.22.2 \  -v /volume longhornio/longhorn-engine:&amp;lt;tag&amp;gt; \  longhorn replica --listen 192.168.22.2:9502 --size 10g /volume Start another replica:  docker run --net longhorn-network --ip 192.168.22.3 \  -v /volume longhornio/longhorn-engine:&amp;lt;tag&amp;gt; \  longhorn replica --listen 192.168.22.3:9502 --size 10g /volume In another terminal, start the controller:  docker run --net longhorn-network --ip 192.</description>
    </item>
    
    <item>
      <title>Test uninstallation</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/uninstallation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/uninstallation/</guid>
      <description>Stability of uninstallation   Launch Longhorn system.
  Use scripts to continuously create then delete multiple DaemonSets.
 e.g., putting the following python test into the manager integration test directory and run it:  from common import get_apps_api_client # NOQA def test_uninstall_script(): apps_api = get_apps_api_client() while True: for i in range(10): name = &amp;quot;ds-&amp;quot; + str(i) try: ds = apps_api.read_namespaced_daemon_set(name, &amp;quot;default&amp;quot;) if ds.status.number_ready == ds.status.number_ready: apps_api.delete_namespaced_daemon_set(name, &amp;quot;default&amp;quot;) except Exception: apps_api.</description>
    </item>
    
    <item>
      <title>Test Version Bump of Kubernetes, API version group, CSI component&#39;s dependency version</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test_version_bump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test_version_bump/</guid>
      <description>GitHub issue: https://github.com/longhorn/longhorn/issues/2757
Test with specific Kubernetes version  For each Kubernetes version (1.18, 1.19, 1.20, 1.21, 1.22), test basic functionalities of Longhorn v1.2.0 (create/attach/detach/delete volume/backup/snapshot using yaml/UI)  Test Kubernetes and Longhorn upgrade  Deploy K3s v1.21 Deploy Longhorn v1.1.2 Create some workload pods using Longhorn volumes Upgrade Longhorn to v1.2.0 Verify that everything is OK Upgrade K3s to v1.22 Verify that everything is OK  Retest the Upgrade Lease Lock We remove the client-go patch https://github.</description>
    </item>
    
    <item>
      <title>Testing ext4 with custom fs params1 (no 64bit, no metadata_csum)</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-1/</guid>
      <description> set the following filesystem parameters: -O ^64bit,^metadata_csum create a volume + pv + pvc with filesystem ext4 named ext4-no-ck-no-64 create a deployment that uses ext4-no-ck-no-64 verify that the pod enters running state and the volume is accessible  </description>
    </item>
    
    <item>
      <title>Testing ext4 with custom fs params2 (no metadata_csum)</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-2/</guid>
      <description> set the following filesystem parameters: -O ^metadata_csum create a volume + pv + pvc with filesystem ext4 named ext4-no-ck create a deployment that uses ext4-no-ck verify that the pod enters running state and the volume is accessible  </description>
    </item>
    
    <item>
      <title>Testing ext4 without custom fs params</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-no-custom-fs-params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-no-custom-fs-params/</guid>
      <description> create a volume + pv + pvc with filesystem ext4 named ext-ck-fail create a deployment that uses ext-ck-fail verify MountVolume.SetUp failed for volume &amp;quot;ext4-ck-fails&amp;quot; is part of the pod events verify that the pod does not enter running state  </description>
    </item>
    
    <item>
      <title>Testing xfs after custom fs params (xfs should ignore the custom fs params)</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/xfs-after-custom-fs-params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/xfs-after-custom-fs-params/</guid>
      <description> create a volume + pv + pvc with filesystem xfs named xfs-ignores create a deployment that uses xfs-ignores verify that the pod enters running state and the volume is accessible  </description>
    </item>
    
    <item>
      <title>Uninstallation Checks</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/uninstallation/uninstallation-checks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/uninstallation/uninstallation-checks/</guid>
      <description>Prerequisites  Have a setup of Longhorn installed on a kubernetes cluster. Have few volumes backups stored on S3/NFS backup store. Have one DR volume created (not activated) in another cluster with a volume in current cluster.  Test steps  Uninstall Longhorn. Check the logs of the job longhorn-uninstall, make sure there is no error. Check all the components of Longhorn from the namespace longhorn-system are uninstalled. E.g. Longhorn manager, Longhorn driver, Longhorn UI, instance manager, engine image, CSI driver etc.</description>
    </item>
    
    <item>
      <title>Upgrade Conflict Handling test</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/upgrade-conflict-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/upgrade-conflict-handling/</guid>
      <description>New installation:  Create a large cluster of many nodes (about 30 nodes) Install Longhorn master Create 100 volumes using volume template claim in statefulSet. Have the backup store configured and create some backups. Set some recurring jobs in the cluster every 1 minute. Observe the setup for 1/2 an hr. Do some operation like attaching detaching the volumes. Verify there is no error in the Longhorn manager.  Upgrading from old version:  Repeat the steps from above test case with Longhorn Prior version.</description>
    </item>
    
    <item>
      <title>Upgrade Lease Lock</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.2/upgrade-lease-lock/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.2/upgrade-lease-lock/</guid>
      <description>The time it takes between the Longhorn Manager starting up and the upgrade completing for that Longhorn Manager can be used to determine if the upgrade lock was released correctly:
 Create a fresh Longhorn installation or delete all of the Longhorn Manager Pods in the existing installation. Check the logs for the Longhorn Manager Pods and note the timestamps for the first line in the log and the timestamp for when the upgrade has completed.</description>
    </item>
    
    <item>
      <title>Upgrade Longhorn with modified Storage Class</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</guid>
      <description>Intro Longhorn can be upgraded with modified Storage Class.
Related Issue https://github.com/longhorn/longhorn/issues/1527
Test steps: Kubectl apply -f  Install Longhorn v1.0.2 kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.0.2/deploy/longhorn.yaml  Create a statefulset using longhorn storageclass for PVCs. Set the scale to 1. Observe that there is a workload pod (pod-1) is using 1 volume (vol-1) with 3 replicas. In Longhorn repo, on master branch. Modify numberOfReplicas: &amp;quot;1&amp;quot; in https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml. Upgrade Longhorn to master by running kubectl apply -f https://raw.</description>
    </item>
    
    <item>
      <title>Volume Deletion UI Warnings</title>
      <link>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/ui-volume-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/ui-volume-deletion/</guid>
      <description>A number of cases need to be manually tested in longhorn-ui. To test these cases, create the Volume with the specified conditions in each case, and then try to delete it. What is observed should match what is described in the test case:
 A regular Volume. Only the default deletion prompt should show up asking to confirm deletion. A Volume with a Persistent Volume. The deletion prompt should tell the user that there is a Persistent Volume that will be deleted along with the Volume.</description>
    </item>
    
  </channel>
</rss>
