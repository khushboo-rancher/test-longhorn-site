<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>
      Longhorn Manual Test Cases
    </title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
  </head>
  <body class="markdown-body" style="display: flex; padding: 1%;">
    
<aside style="width: 30%; padding: 1%; border-right: 1px solid lightgray;">
  <a href="https://khushboo-rancher/test-longhorn-site/manual"><h2>Manual Test Cases</h2></a>
  
  <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/">End-to-end test cases</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/deployment/"> Deployment of Longhorn  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/ui/">UI  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/volume/">Volume  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/high-availability/">High Availability  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/kubernetes/">Kubernetes  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/backup/">Backup  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/node/">Node  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/scheduling/">Scheduling  </a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/end-to-end-test-cases/upgrade/">Upgrade  </a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/">Pre-release tests</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/">Air Gap</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/air-gap-installation/">Air gap installation</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/air-gap/air-gap-instance-manager-name/">Air gap installation with an instance-manager-image name longer than 63 characters</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/">Backup &amp; Restore tests</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/dr-volume-live-upgrade-and-rebuild/">#1279 DR volume live upgrade and rebuild</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/concurrent-backup-creation-deletion/">#1326 concurrent backup creation &amp; deletion</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/concurrent-backup/">#1341 concurrent backup test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/restore-volume-node-down/">#1355 The node the restore volume attached to is down</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/dr-volume-node-rebooted/">#1366 &amp;&amp; #1328 The node the DR volume attached to is rebooted</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/google-cloud-s3-interop-backups/">#1404 test backup functionality on google cloud and other s3 interop providers.</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/backup-and-restore/backup-block-deletion/">#1431 backup block deletion test</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/basic-operations-parallelism/">Basic operations parallelism</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/basic-operations-parallelism/snapshot-while-writing-data/">Snapshot while writing data in the volume</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/">Cluster Restore</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/system-upgrade-with-restore/">Longhorn system upgrade with restore</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/node-creation-deletion-with-restore/">Node creation and deletion with restore</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/cluster-restore/recover-volumes-after-restore/">Recover Longhorn volumes after Rancher cluster restore</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/">Environment</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/cluster-using-customized-kubelet-root-directory/">Cluster using customize kubelet root directory</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/k3s-selinux-compatibility/">Compatibility with k3s and SELinux</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/environment/rke2-cis-1.5-profile/">Test Longhorn Deployment on RKE2 with CIS-1.5 profile</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/">HA</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/backing-image-error-reporting-and-retry/">Backing Image Error Reporting and Retry</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/disk-migration-in-aws-asg/">Disk migration in AWS ASG</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/partial-engine-deployment/">Longhorn with engine is not deployed on all the nodes</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/replica-rebuilding/">Replica Rebuilding</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/ha/single-replica-node-down/">Single replica node down</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/">Node</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/backing-image-on-a-down-node/">Backing Image on a down node</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/degraded-availability/">Degraded availability with added nodes</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/improve-node-failure-handling/">Improve Node Failure Handling By Automatically Force Delete Terminating Pods of StatefulSet/Deployment On Downed Node</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-disconnection/">Node disconnection test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-drain-deletion/">Node drain and deletion test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/physical-node-down/">Physical node down</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/kubelet-restart-on-a-node/">Test kubelet restart on a node of the cluster</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/node/node-deletion/">Test node deletion</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/">Resiliency</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/simulated-slow-disk/">#2206 Fix the spinning disk on Longhorn</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/resiliency/timeout/">Test timeout on loss of network connectivity</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/stability/">Stability</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/stability/multiple-installation/">Longhorn installation multiple times</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/stress/backup-listing/">Test backup listing S3/NFS</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/uninstallation/uninstallation-checks/">Uninstallation Checks</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/">Upgrade</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/auto-upgrade-engine/">Automatically Upgrading Longhorn Engine Test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/kubernetes-upgrade-test/">Kubernetes upgrade test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/longhorn-upgrade-test/">Longhorn Upgrade test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/update_csi_components_when_images_change/">Re-deploy CSI components when their images change</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/backing-image-during-upgrade/">Test Backing Image during Longhorn upgrade</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/engine-crash-during-live-upgrade/">Test Engine Crash During Live Upgrade</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/upgrade-with-new-instance-manager/">Test System Upgrade with New Instance Manager</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/pre-release/upgrade/upgrade-conflict-handling/">Upgrade Conflict Handling test</a></li>
  

</ul>

  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/">Release specific tests</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/">v1.0.0</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/new-node-custom-data-directory/">New Node with Custom Data Directory</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/">Operating System specific tests for SUSE SLES12SP3</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-1/">Testing ext4 with custom fs params1 (no 64bit, no metadata_csum)</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-2/">Testing ext4 with custom fs params2 (no metadata_csum)</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-no-custom-fs-params/">Testing ext4 without custom fs params</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.0/suse-sles12sp3/xfs-after-custom-fs-params/">Testing xfs after custom fs params (xfs should ignore the custom fs params)</a></li>
  

</ul>

  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/">v1.0.1</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/besteffort-recurring-job/">BestEffort Recurring Job Cleanup</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/change-imagepullpolicy/">Change imagePullPolicy to IfNotPresent Test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/">DR volume related latest backup deletion test</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/nfsv4-enforcement/">NFSv4 Enforcement (No NFSv3 Fallback)</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/priorityclass-default-setting/">Priority Class Default Setting</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/error-fail-remount/">Return an error when fail to remount a volume</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/test-s3-access-style/">Test access style for S3 compatible backupstore</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/test-s3-backupstore/">Test S3 backupstore in a cluster sitting behind a Http Proxy</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.1/ui-volume-deletion/">Volume Deletion UI Warnings</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.2/">v1.0.2</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.0.2/upgrade-lease-lock/">Upgrade Lease Lock</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/">v1.1.0</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/prometheus_support/">Prometheus Support</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/">Recurring backup job interruptions</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/">Reusing failed replica for rebuilding</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/kubelet_volume_metrics/">Support Kubelet Volume Metrics</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/additional-printer-columns/">Test Additional Printer Columns</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/instance-manager-ip-sync/">Test Instance Manager IP Sync</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/iscsi_installation/">Test ISCSI Installation on EKS</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/rwx_feature/">Test Read Write Many Feature</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/uninstallation/">Test uninstallation</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/">Upgrade Longhorn with modified Storage Class</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/">v1.1.1</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/csi-sanity-check/">CSI Sanity Check</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/partial-engine-deployment/">Longhorn with engine is not deployed on all the nodes</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/">Set Tolerations/PriorityClass For System Components</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/disable_ipv6/">Test Disable IPv6</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-file-sync-cancellation/">Test File Sync Cancellation</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/ws-traffic-flood/">Test Frontend Traffic</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/delete-node/">Test Node Delete</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-node-selector/">Test Node Selector</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/">Test RWX share-mount ownership reset</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/test-service-account-mount/">Test Service Account mount on host</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/snapshot-purge-error-handling/">Test Snapshot Purge Error Handling</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/">Test system upgrade with the deprecated CPU setting</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/">v1.1.2</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/delete-cronjob-for-detached-volumes/">Test CronJob For Volumes That Are Detached For A Long Time</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/full-ws-data-tranfer-when-no-updates/">Test Frontend Web-socket Data Transfer When Resource Not Updated</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.1.2/instance-manager-streaming-connection-recovery/">Test Instance Manager Streaming Connection Recovery</a></li>
  

</ul>

  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/">v1.2.0</a></li>
  
    <ul>

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test-backing-image-upload/">Test backing image</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/backup-creation-with-old-engine-image/">Test Backup Creation With Old Engine Image</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test-instance-manager-cleanup-during-uninstall/">Test instance manager cleanup during uninstall</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/label-driven-recurring-job/">Test Label-driven Recurring Job</a></li>
  

  <li><a href="https://khushboo-rancher/test-longhorn-site/manual/release-specific/v1.2.0/test_version_bump/">Test Version Bump of Kubernetes, API version group, CSI component&rsquo;s dependency version</a></li>
  

</ul>

  

</ul>

  

</ul>

  
</aside>

<main style="padding: 1%; width: 70%;">
  <h1 id="title"><ol start="3">
<li>Volume</li>
</ol>
</h1>
  <div>
    <article id="content">
       <h3 id="test-cases-for-volume">Test cases for Volume</h3>
<table>
<thead>
<tr>
<th><strong>#</strong></th>
<th><strong>Test Case</strong></th>
<th><strong>Test Instructions</strong></th>
<th><strong>Expected Results</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Check volume Details</td>
<td><strong>Prerequisite:</strong><br><br>*   Longhorn Nodes has node tags<br>*   Node Disks has disk tags<br>*   Backup target is set to NFS server, or S3 compatible target<br><br>1.  Create a workload using Longhorn volume<br>2.  Check volume details page<br>3.  Create volume backup</td>
<td>*   Volume Details<br>    *   <code>State</code> should be <code>Attached</code><br>    *   <code>Health</code> should be healthy<br>    *   <code>Frontend</code> should be <code>Block Device</code><br>    *   <code>Attached Node &amp; Endpoint</code> should be node name that volume is attached to and PATH of the volume device file on that node.<br>    *   <code>Size</code> should match volume size specified in Create Volume step<br>    *   <code>Actual Size</code> should be <code>0Bi</code> (No data has been written to the volume yet)<br>    *   <code>Engine Image</code> should be <code>longhornio/longhorn-engine:&lt;LONGHORN_VERSION&gt;</code><br>    *   <code>Created</code> should indicate time since volume is created.<br>    *   <code>Node Tags</code> should be empty (no node tags has been specified during creation)<br>    *   <code>Disk Tags</code> should be empty (no disk tags has been specified during creation)<br>    *   <code>Last Backup</code> should be empty (no backup has been created)<br>    *   <code>Last Backup At</code> should be empty (no backup has been created)<br>    *   <code>Instance Manager</code> should contain instance manager image name<br>    *   <code>Namespace</code> should match namespace specified in Volume Create step.<br>    *   <code>PVC Name</code> should be empty (no PV has been created for that volume yet)<br>    *   <code>PV Name</code> should be empty (no PV has been created for that volume yet)<br>    *   <code>PV Status</code> should be empty (no PV/PVC has been created for that volume yet)</td>
</tr>
<tr>
<td>2</td>
<td>Filter Volumes</td>
<td>User should be able to filter volumes using the following filters<br><br>*   Name<br>*   Node<br>*   Status (Healthy, In progress, Degraded, Faulted, detached)<br>*   Namespace<br>*   Node redundancy (Yes, Limited, No)<br>*   PV Name<br>*   PVC Name<br>*   Node tag<br>*   Disk tag<br><br>Notes:<br><br>*   Limited node redundancy: at least one healthy replica is running at the same node as another</td>
<td>*   Volume list should match filtering criteria applied.</td>
</tr>
<tr>
<td>3</td>
<td>Delete multiple volumes</td>
<td>*   <strong>Prerequisite:</strong><br>    *   Create multiple volumes<br><br>1.  Select multiple volumes and delete</td>
<td>*   Volumes should be deleted</td>
</tr>
<tr>
<td>4</td>
<td>Attach multiple volumes</td>
<td>*   <strong>Prerequisite:</strong><br>    *   Create multiple volumes<br><br>1.  Select multiple volumes and Attach them to a node</td>
<td>*   All Volumes should be attached to the same node specified in volume attach request.</td>
</tr>
<tr>
<td>5</td>
<td>Attach multiple volumes in maintenance mode</td>
<td>*   <strong>Prerequisite:</strong><br>    *   Create multiple volumes<br><br>1.  Select multiple volumes and Attach them to a node in maintenance mode</td>
<td>*   All Volumes should be attached in maintenance mode to the same node specified in volume attach request.</td>
</tr>
<tr>
<td>6</td>
<td>Detach multiple volumes</td>
<td>*   <strong>Prerequisite:</strong><br>    *   Multiple attached volumes<br>*   Select multiple volumes and detach</td>
<td>*   Volumes should be detached</td>
</tr>
<tr>
<td>7</td>
<td>Backup multiple Volumes</td>
<td>*   <strong>Prerequisite:</strong><br>    *   Longhorn should be configured to point to a backupstore<br>    *   Multiple volumes existed and attached to node/used buy kubernetes workload<br>    *   Write some data to multiple volumes and compute it’s checksum<br>*   Select multiple volumes and Create a backup<br>*   restore volumes backups and check its data checksum</td>
<td>*   Volume backups should be created<br>*   Restored volumes from backup should contain the same data when backup is created</td>
</tr>
<tr>
<td>8</td>
<td>Create PV/PVC for multiple volumes</td>
<td><strong>Prerequisite:</strong><br><br>*   Create multiple volumes<br><br>1.  Select multiple volumes<br>2.  Create a PV, specify filesysem<br>3.  Check PV in Lonhgorn UI and in Kubernetes<br>4.  Create PVC<br>5.  Check PVC in Lonhgorn UI and in Kubernetes<br>6.  Delete PVC<br>7.  Check PV in Lonhgorn UI and in Kubernetes</td>
<td>*   For all selected volumes<br>    *   PV should created<br>    *   PV/PVC status in UI should be <code>Available</code><br>    *   PV <code>spec.csi.fsType</code> should match filesystem specified in PV creation request<br>    *   PV <code>spec.storageClassName</code> should match the setting in <code>Default Longhorn Static StorageClass Name</code><br>    *   PV <code>spec.csi.volumeHandle</code> should be the volume name<br>    *   PV/PVC status in UI should be <code>Bound</code> in Longhorn UI<br>    *   PVC namespace should match namespace specified in PVC creation request<br>    *   After Deleting PVC, PV/PVC status should be <code>Relased</code> in Longhorn UI.</td>
</tr>
<tr>
<td>9</td>
<td>Volume expansion</td>
<td>Check Multiple Volume expansion test cases work for multiple volumes<br><br><a href="https://rancher.atlassian.net/wiki/spaces/LON/pages/354453117/Volume+detail+page">Test Cases in Volume Details page</a></td>
<td>Volume expansion should work for multiple volumes.</td>
</tr>
<tr>
<td>10</td>
<td>Engine Offline Upgrade For Multiple Volumes</td>
<td><strong>Prerequisite:</strong><br><br>*   Volume is consumed by Kubernetes deployment workload<br>*   Volume use old Longhorn Engine<br><br>1.  Write data to volume, compute it’s checksum (checksum#1)<br>2.  Scale down deployment , volume gets detached<br>3.  Upgrade Longhorn engine image to use new deployed engine image<br>4.  Scale up deployment, volume gets attached</td>
<td>*   Volume read/write operations should work before and after engine upgrade.<br>*   Old Engine <code>Reference Count</code> will be decreased by 1<br>*   New Engine <code>Reference Count</code> will be increased by 1</td>
</tr>
<tr>
<td>12</td>
<td>Show System Hidden</td>
<td><strong>Prerequisite</strong>:<br><br>*   Volume is created and attached to a pod.<br><br>1.  Click the volume appearing on volume list page, it takes user to volume.<br>2.  Take snapshot and upgrade the replicas.<br>3.  Under snapshot section, enable option &lsquo;Show System Hidden</td>
<td>Enabling this option will show system created snapshots while rebuilding of replicas.</td>
</tr>
<tr>
<td>13</td>
<td>Event log</td>
<td><strong>Prerequisite</strong>:<br><br>*   Volume is created and attached to a pod.<br><br>1.  Click event log to expand</td>
<td>Verify details appearing in logs.</td>
</tr>
</tbody>
</table>
<h2 id="automation-tests">Automation Tests</h2>
<table>
<thead>
<tr>
<th><strong>#</strong></th>
<th><strong>Test name</strong></th>
<th><strong>Description</strong></th>
<th><strong>Tags</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>test_attach_without_frontend</td>
<td>Test attach in maintenance mode (without frontend)<br><br>1.  Create a volume and attach to the current node with enabled frontend<br>2.  Check volume has <code>blockdev</code><br>3.  Write <code>snap1_data</code> into volume and create snapshot <code>snap1</code><br>4.  Write more random data into volume and create another anspshot<br>5.  Detach the volume and reattach with disabled frontend<br>6.  Check volume still has <code>blockdev</code> as frontend but no endpoint<br>7.  Revert back to <code>snap1</code><br>8.  Detach and reattach the volume with enabled frontend<br>9.  Check volume contains data <code>snap1_data</code></td>
<td>Volume</td>
</tr>
<tr>
<td>2</td>
<td>test_volume_basic</td>
<td>Test basic volume operations:<br><br>1.  Check volume name and parameter<br>2.  Create a volume and attach to the current node, then check volume states<br>3.  Check soft anti-affinity rule<br>4.  Write then read back to check volume data</td>
<td>Volume</td>
</tr>
<tr>
<td>3</td>
<td>test_volume_iscsi_basic</td>
<td>Test basic volume operations with iscsi frontend<br><br>1.  Create and attach a volume with iscsi frontend<br>2.  Check the volume endpoint and connect it using the iscsi initator on the node.<br>3.  Write then read back volume data for validation</td>
<td>Volume</td>
</tr>
<tr>
<td>4</td>
<td>test_volume_multinode</td>
<td>Test the volume can be attached on multiple nodes<br><br>1.  Create one volume<br>2.  Attach it on every node once, verify the state, then detach it</td>
<td>Volume</td>
</tr>
<tr>
<td>5</td>
<td>test_volume_update_replica_count</td>
<td>Test updating volume&rsquo;s replica count<br><br>1.  Create a volume with 3 replicas<br>2.  Attach the volume<br>3.  Increase the replica to 5.<br>4.  Volume will become degraded and start rebuilding<br>5.  Wait for rebuilding to complete<br>6.  Update the replica count to 2. Volume should remain healthy<br>7.  Remove 3 replicas, so there will be 2 replicas in the volume<br>8.  Verify the volume is still healthy</td>
<td>Volume</td>
</tr>
<tr>
<td>6</td>
<td>test_ha_prohibit_deleting_last_replica</td>
<td>Test prohibiting deleting the last replica<br><br>1.  Create volume with one replica and attach to the current node.<br>2.  Try to delete the replica. It should error out</td>
<td>Volume</td>
</tr>
<tr>
<td>7</td>
<td>test_replica_datapath_cleanup</td>
<td>Test replicas data path cleanup<br><br>1.  Create host disk <code>extra_disk</code> and add it to the current node.<br>2.  Disable all the disks except for the ones on the current node.<br>3.  Create a volume with 5 replicas (soft anti-affinity on)<br>    1.  To make sure both default disk and extra disk can have one replica<br>    2.  Current we don&rsquo;t have anti-affinity for disks on the same node<br>4.  Verify the data path for replicas are created.<br>5.  Delete the volume.<br>6.  Verify the data path for replicas are deleted.</td>
<td>Volume</td>
</tr>
<tr>
<td>8</td>
<td>test_expansion_basic</td>
<td>Test volume expansion using Longhorn API<br><br>1.  Create volume and attach to the current node<br>2.  Generate data <code>snap1_data</code> and write it to the volume<br>3.  Create snapshot <code>snap1</code><br>4.  Expand the volume (volume will be detached, expanded, then attached)<br>5.  Verify the volume has been expanded<br>6.  Generate data <code>snap2_data</code> and write it to the volume<br>7.  Create snapshot <code>snap2</code><br>8.  Gerneate data <code>snap3_data</code> and write it after the original size<br>9.  Create snapshot <code>snap3</code> and verify the <code>snap3_data</code> with location<br>10.  Detach and reattach the volume.<br>11.  Verify the volume is still expanded, and <code>snap3_data</code> remain valid<br>12.  Detach the volume.<br>13.  Reattach the volume in maintence mode<br>14.  Revert to <code>snap2</code> and detach.<br>15.  Attach the volume and check data <code>snap2_data</code><br>16.  Generate <code>snap4_data</code> and write it after the original size<br>17.  Create snapshot <code>snap4</code> and verify <code>snap4_data</code>.<br>18.  Detach the volume and revert to <code>snap1</code><br>19.  Validate <code>snap1_data</code></td>
<td>Volume: Expansion</td>
</tr>
<tr>
<td>9</td>
<td>test_restore_inc_with_expansion</td>
<td>Run test against a random backupstores<br><br>1.  Create a volume and attach to the current node<br>2.  Generate <code>data0</code>, write to the volume, make a backup <code>backup0</code><br>3.  Create three DR(standby) volumes from the backup: <code>dr_volume0/1/2</code><br>4.  Wait for all three DR volumes to finish the initial restoration<br>5.  Verify DR volumes&rsquo;s <code>lastBackup</code> is <code>backup0</code><br>6.  Verify snapshot/pv/pvc/change backup target are not allowed as long as the DR volume exists<br>7.  Activate standby <code>dr_volume0</code> and attach it to check the volume data<br>8.  Expand the original volume. Make sure the expansion is successful.<br>9.  Generate <code>data1</code> and write to the original volume and create <code>backup1</code><br>10.  Make sure <code>dr_volume1</code>&lsquo;s <code>lastBackup</code> field has been updated to <code>backup1</code><br>11.  Activate <code>dr_volume1</code> and check data <code>data0</code> and <code>data1</code><br>12.  Generate <code>data2</code> and write to the original volume after original SIZE<br>13.  Create <code>backup2</code><br>14.  Wait for <code>dr_volume2</code> to finish expansion, show <code>backup2</code> as latest<br>15.  Activate <code>dr_volume2</code> and verify <code>data2</code><br>16.  Detach <code>dr_volume2</code><br>17.  Create PV, PVC and Pod to use <code>sb_volume2</code>, check PV/PVC/POD are good</td>
<td>Volume: Expansion<br><br>Backup: Disaster Recovery</td>
</tr>
<tr>
<td>10</td>
<td>test_snapshot</td>
<td>Test snapshot operations<br><br>1.  Create a volume and attach to the node<br>2.  Create the empty snapshot <code>snap1</code><br>3.  Generate and write data <code>snap2_data</code>, then create <code>snap2</code><br>4.  Generate and write data <code>snap3_data</code>, then create <code>snap3</code><br>5.  List snapshot. Validate the snapshot chain relationship<br>6.  Mark <code>snap3</code> as removed. Make sure volume&rsquo;s data didn&rsquo;t change<br>7.  List snapshot. Make sure <code>snap3</code> is marked as removed<br>8.  Detach and reattach the volume in maintenance mode.<br>9.  Make sure the volume frontend is still <code>blockdev</code> but disabled<br>10.  Revert to <code>snap2</code><br>11.  Detach and reattach the volume with frontend enabled<br>12.  Make sure volume&rsquo;s data is <code>snap2_data</code><br>13.  List snapshot. Make sure <code>volume-head</code> is now <code>snap2</code>&rsquo;s child<br>14.  Delete <code>snap1</code> and <code>snap2</code><br>15.  Purge the snapshot.<br>16.  List the snapshot, make sure <code>snap1</code> and <code>snap3</code> are gone. <code>snap2</code> is marked as removed.<br>17.  Check volume data, make sure it&rsquo;s still <code>snap2_data</code>.</td>
<td>Volume: Snapshot</td>
</tr>
<tr>
<td>11</td>
<td>test_kubernetes_status</td>
<td>Test Volume feature: Kubernetes Status<br><br>1.  Create StorageClass with <code>reclaimPolicy = Retain</code><br>2.  Create a statefulset <code>kubernetes-status-test</code> with the StorageClass<br>    1.  The statefulset has scale of 2.<br>3.  Get the volume name from the SECOND pod of the StateufulSet pod and create an <code>extra_pod</code> with the same volume on the same node<br>4.  Check the volumes that used by the StatefulSet<br>    1.  The volume used by the FIRST StatefulSet pod will have one workload<br>    2.  The volume used by the SECOND StatefulSet pod will have two workloads<br>    3.  Validate related status, e.g. pv/pod name/state, workload name/type<br>5.  Check the volumes again<br>    1.  PV/PVC should still be bound<br>    2.  The volume used by the FIRST pod should have history data<br>    3.  The volume used by the SECOND and extra pod should have current data point to the extra pod<br>6.  Delete the extra pod<br>    1.  Now all the volume&rsquo;s should only have history data(<code>lastPodRefAt</code> set)<br>7.  Delete the PVC<br>    1.  PVC should be updated with status <code>Released</code> and become history data<br>8.  Delete PV<br>    1.  All the Kubernetes status information should be cleaned up.<br>9.  Reuse the two Longhorn volumes to create new pods<br>    1.  Since the <code>reclaimPolicy == Retain</code>, volume won&rsquo;t be deleted by Longhorn<br>    2.  Check the Kubernetes status now updated, with pod info but empty workload<br>    3.  Default Longhorn Static StorageClass will remove the PV with PVC, but leave Longhorn volume</td>
<td>Volume: Kubernetes Status</td>
</tr>
</tbody>
</table>
<h2 id="replica">Replica</h2>
<table>
<thead>
<tr>
<th><strong>#</strong></th>
<th><strong>Test Case</strong></th>
<th><strong>Test Instructions</strong></th>
<th><strong>Expected Results</strong></th>
<th><strong>Automated ? / test name</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Replica list</td>
<td>1.  Create a volume and change the default number of replicas<br>2.  Attach volume to a node<br>3.  Check replica list</td>
<td>*   Number of replicas should match number specified in volume creation request<br>*   All replicas should be <code>Running</code>, and <code>Healthy</code><br>*   Replica info also should contain, <code>Node Name</code>, <code>Replica Instance Manager Name</code>, <code>Replica Path</code></td>
<td>test_volume_update_replica_count</td>
</tr>
<tr>
<td>2</td>
<td>Update volume replica count (increase)</td>
<td>1.  Create a volume<br>2.  Attach volume to a node<br>3.  Increase replica count +1</td>
<td>*   New system hidden snapshot should be created<br>*   A new replica should be created<br>*   New replicas should be <code>Running</code> &amp; <code>Rebuilding</code></td>
<td>test_volume_update_replica_count</td>
</tr>
<tr>
<td>3</td>
<td>Update volume replica count (decrease)</td>
<td>1.  Create a volume<br>2.  Attach volume to a node<br>3.  decrease replica count by +1<br>4.  Delete a replica</td>
<td>*   After decreasing replica count, nothing should happen<br>*   Deleting a replica will not trigger replica rebuild</td>
<td>test_volume_update_replica_count</td>
</tr>
</tbody>
</table>
<h2 id="snapshot">Snapshot</h2>
<table>
<thead>
<tr>
<th><strong>#</strong></th>
<th><strong>Test Case</strong></th>
<th><strong>Test Instructions</strong></th>
<th><strong>Expected Results</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Create Snapshot</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)</td>
<td>*   Volume head should have parent as snapshot#1<br>*   Snapshot should be created</td>
</tr>
<tr>
<td>2</td>
<td>Revert Snapshot</td>
<td>1.  Create a deployment workload with <code>nReplicas = 1</code> using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Write some other data, compute it’s checksum (checksum#2)<br>4.  Create a snapshot (snapshot#1)<br>5.  Scale down deployment <code>nReplicas = 0</code><br>6.  Attach volume in <code>maintenance mode</code><br>7.  Revert to (snapshot#1)<br>8.  Detach volume<br>9.  Scale back deployment <code>nReplicas = 1</code><br>10.  Compute data checksum (checksum#3)</td>
<td>*   Volume head should have parent as snapshot#1<br>*   Volume state should be <code>Detached</code> after scaling down deployment <code>nReplicas = 0</code><br>*   In Volume Details <code>Attached Node</code> should be Node name which volume is attached to, without an <code>Endpoint</code> (block device path)<br>*   Data checksum after revert should match data checksum when taking snapshot <code>checksum#3 == checksum#1</code></td>
</tr>
<tr>
<td>3</td>
<td>Delete Snapshot</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Delete (snapshot#2)<br>7.  Revert to (snapshot#3)<br>8.  Revert to (snapshot#1)</td>
<td>*   Snapshot#2 will be deleted, verify in replica /var/lib/rancher/longhorn/replicas/<br>*   After reverting to snapshot#3 verify the data.<br>*   After reverting to snapshot#1 data checksum should match (checksum#1)</td>
</tr>
<tr>
<td>4</td>
<td>Delete Snapshot while rebuilding replicas</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Delete a replica<br>5.  while replica is rebuilding, try to delete (snapshot#1)</td>
<td>*   New system snapshot should be created<br>*   Will **NOT** be able to delete snapshot#1</td>
</tr>
<tr>
<td>5</td>
<td>Snapshot Purge</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Delete a replica<br>5.  After rebuild is complete, delete (snapshot#1)</td>
<td>*   New system snapshot should be created<br>*   Snapshot#1 will be delete<br>*   Snapshot purge process will be triggered<br>*   Only system snapshot should be present<br>*   You will not be able revert to the system snapshot</td>
</tr>
<tr>
<td>6</td>
<td>Create recurring snapshots</td>
<td>1.  Create a deployment workload with <code>nReplicas = 1</code> using Longhorn volume<br>2.  Write data to volume , compute it’s checksum (checksum#1)<br>3.  Create a recurring snapshot <code>every 5 minutes</code>. and set retain count to <code>5</code><br>4.  Wait for 2 recurring snapshots to triggered (snapshot#1, snapshot#2 )<br>5.  Scale down deployment <code>nReplicas = 0</code><br>6.  Attach volume in <code>maintenance mode</code><br>7.  Revert to (snapshot#1)<br>8.  Scale back deployment <code>nReplicas = 1</code><br>9.  Wait for another recurring snapshots to triggered (snapshot#3)<br>10.  Delete (snapshot#1)</td>
<td>*   Snapshots (snapshot#1, snapshot#2) should be created<br>*   Before deleting (snapshot#1), Parent snapshot of (snapshot#3) should be (snapshot#1)<br>*   After deleting (snapshot#1), Parent snapshot of (snapshot#3) should be starting point.<br>*   Only max of <code>5</code> snapshots should be retained<br>*   Oldest snapshot will be removed when number of snapshots created by recurring job exceeds retain count<br>*   only snapshots generated by recurring job is affected by retain count, user can created manual snapshots and it will not be deleted automatically.</td>
</tr>
<tr>
<td>8</td>
<td>Disabling/Deleting recurring snapshots</td>
<td>1.  Create a deployment workload with <code>nReplicas = 1</code> using Longhorn volume<br>2.  Write data to volume , compute it’s checksum (checksum#1)<br>3.  Create a recurring snapshot <code>every 5 minutes</code>. and set retain count to <code>5</code><br>4.  Wait for 2 recurring snapshots to triggered (snapshot#1,snapshot#2 )<br>5.  Delete the recurring snapshots</td>
<td>*   Recurring snapshots will stop after deletion of it.<br>*   Existing snapshots should retain.<br>*   User should be able to take snapshot manually.</td>
</tr>
<tr>
<td>9</td>
<td>Operation with volume created using rancher UI</td>
<td>1.  Create pv/pvc in rancher UI.<br>2.  Deploy a workload with PVC created in rancher UI.<br>3.  Write data to volume, compute it’s checksum (checksum#1)<br>4.  In longhorn UI, create a snapshot.<br>5.  Write data to volume again.<br>6.  Revert to snapshot.<br>7.  Delete the snapshot.</td>
<td>*   User should be able to create snapshot.<br>*   User should to revert to snapshot created verify this by checksum.<br>*   User should be able to delete the snapshot. Verify this in replicas in the nodes.</td>
</tr>
<tr>
<td>10</td>
<td>Delete the last snapshot</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Delete (snapshot#3)</td>
<td>*   Data from last snapshot should not get lost.</td>
</tr>
<tr>
<td>11</td>
<td>Multi branch snapshot</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Revert to snapshot#2<br>7.  Write data to volume, compute it’s checksum.<br>8.  Repeat steps 2,3 two more times to have (snapshot#4, snapshot#5) with different data files (checksum#5, checksum#6)<br>9.  Revert to snapshot#3</td>
<td>*   Verify the data in any of the replica</td>
</tr>
<tr>
<td>12</td>
<td>Backup from a snapshot</td>
<td>1.  Create a workload using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Take backup from a snapshot#2.<br>7.  Restore from the backup taken</td>
<td>Verify the data of the backup, it should match data from snapshot#2</td>
</tr>
</tbody>
</table>
<h2 id="volume-expansion">Volume Expansion</h2>
<table>
<thead>
<tr>
<th><strong>#</strong></th>
<th><strong>Test Case</strong></th>
<th><strong>Test Instructions</strong></th>
<th><strong>Expected Results</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Volume <strong>Online expansion</strong> for attached volume<br><br><strong>(Not Supported for now)</strong></td>
<td>1.  Create multiple 4 volumes, each of size 5 GB, Attach them to nodes<br>2.  Format each volume using one of the following formats (ext 2/3/4 &amp; xfs)<br>3.  Mount volumes to directories on the nodes.<br>4.  Check volume size and used space using <code>df -h</code> command<br>5.  Write 4 GB data file to each volume<br>6.  For each volume, from Operation menu, Click <code>Expand Volume</code>, set size to <code>10 GB</code>, and click <code>OK</code><br>7.  Check volume size and used space using <code>df -h</code> command<br>8.  Add more 4 GB data file to each volume<br>9.  Check volume size and used space using <code>df -h</code> command<br>10.  Check volume size expanded</td>
<td>*   Volumes should be expanded to the new size<br>*   Volume read/write operations should work after size expansion</td>
</tr>
<tr>
<td>2</td>
<td>Volume <strong>Online expansion</strong> for volume consumed by Kubernetes workload<br><br><strong>→ Kubernetes Version: 1.15</strong></td>
<td>1.  Create a <code>2GB</code> volume used by a Kubernetes workload<br>2.  Expand Volume size to <code>10 GB</code><br>3.  In Kubernetes, edit PV/PVC capacity to match new volume size.<br>4.  Check volume size using <code>df -h</code> command from Kubernetes workload<br>5.  Write 8 GB data file to volume, and compute its checksum</td>
<td>*   When resizing, A message indicate that <code>The capacity of related PV and PVC will not be updated</code><br>*   Volumes should be expanded to the new size<br>*   Volume read/write operations should work after size expansion</td>
</tr>
<tr>
<td>3</td>
<td>Volume <strong>Online expansion</strong> for volume consumed by Kubernetes workload<br><br><strong>→ Kubernetes Version: 1.16+</strong></td>
<td><strong>Prerequisite:</strong><br><br>*   PVC is dynamically provisioned by the StorageClass.<br>*   The Kubernetes is version 1.16+ or the feature gate for volume expansion is enabled.<br>*   The StorageClass should support resize, <code>allowVolumeExpansion: true</code> is set in the StorageClass<br><br>1.  Create a <code>2GB</code> volume used by a Kubernetes workload<br>2.  Expand Volume size to <code>10 GB</code><br>3.  Check volume size using <code>df -h</code> command from Kubernetes workload<br>4.  Write 8 GB data file to volume, and compute its checksum</td>
<td>*   When resizing, A message indicate that <code>The capacity of related PV and PVC will not be updated</code><br>*   Volumes should be expanded to the new size<br>*   Volume read/write operations should work after size expansion</td>
</tr>
<tr>
<td>4</td>
<td>Volume Offline expansion<br><br><strong>Kubernetes Version: &lt; 1.16</strong></td>
<td>1.  Create and attach a volume<br>2.  Format and mount the volume. Fill up the volume and get the checksum (checksum#1)<br>3.  Unmount and detach the volume.<br>4.  Expand the volume and wait for the expansion complete.<br>5.  Reattach and remount the volume. Check the checksum and if the filesystem is expanded.<br>6.  Fill up the expanded parts and get the checksum. (checksum#2)<br>7.  Unmount and detach the volume.<br>8.  Launch a workload for it on a different node. Check data checksum</td>
<td>*   Volume size should be expanded<br>*   Volume read/write operations should work after size expansion<br>*   After expansion, data checksum should match checksum#1<br>*   Final data checksum should match checksum#2</td>
</tr>
<tr>
<td>5</td>
<td>Volume expansion with revert and backup</td>
<td>1.  Create and attach a volume.<br>2.  Format and mount the volume. Fill up the volume and get the checksum. (checksum#1)<br>3.  Create the 1st snapshot and backup. (snapshot#1 &amp; backup#1)<br>4.  Expand the volume. Fill up the expanded part and get the checksum (checksum#2)<br>5.  Create the 2nd snapshot and backup. (snapshot#2 &amp; backup#2)<br>6.  Check if the backup volume size is expanded.<br>7.  Restore backup#2 to a volume and check its data<br>8.  Clean up then refill the volume. Get the checksum. (checksum#3)<br>9.  Create the 3rd snapshot and backup. (snapshot#3 &amp; backup#3)<br>10.  Revert to the 2nd snapshot. Check the checksum.<br>11.  Revert to the 1st snapshot. Check the checksum and if we can still use the expanded part.</td>
<td>*   Volume should be expanded<br>*   backup#2 size should be expanded and match volume new expanded size<br>*   Restored volume data from backup#2 should match checksum#2<br>*   After reverting to snapshot#1, data checksum should match checksum#1<br>*   After reverting to snapshot#1 expanded size should be usable.<br>*   Volume read/write operations should work after expansion and revert.</td>
</tr>
</tbody>
</table>

    </article>
  </div>
</main>

    
    
  </body>
</html>
